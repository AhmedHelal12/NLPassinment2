{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+m1J8YJObyW4TdCXyeN9h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedHelal12/NLPassinment2/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qefCn29FGHlA",
        "outputId": "7d787c0a-3c84-4dff-ac7f-3e5b8880d9fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'generated_text': 'deep learning. The first step is designing an algorithm that uses a finite number of discrete methods, which are used in the classifiers, and takes the set of models. The second stage can use some set of constraints that can be used for testing, with some input data to be generated.\\n\\nFinally it will use some different implementations of the set of constraints, and test those with their outputs for correctness. These constraints can be used to test a classifier, though it is important the constraints are'}], [{'generated_text': 'natural language processing of children and their ability to acquire information about them.\\n\\nThere are many reasons to believe that the brain is a \"brain that tells us things,\" and their information is processed into language or images, but that the basic process isn\\'t. At least that\\'s the view of neuroscientist Thomas M. Friedman, an author of the best-selling \"Brain, Language, and Psychology: The Scientific and Psychological Evidence\" and coauthor of the book \"The Brain, Language,'}], [{'generated_text': \"computer vision system.\\n\\nOn Monday, we announced that Uber was partnering officially with the University of North Carolina at Chapel Hill to become the first startup to have distributed autonomous driving technology. The announcement followed a wave of buzz about Uber's success in California through a recent $100 million grant to Harvard University, and it certainly does not bode well for future Uber-driving projects.\\n\\nOn its end, the company will have two projects in the works — a new virtual-reality project and a\"}], [{'generated_text': 'history of egypt.\" Geoecology, 66, 1077 – 1083 (1998). We have identified a number of unusual and rare geodesy events that occurred on the northern coasts of the North American and Eurasian seas from 1882 to the present day (Munro and Crespo, 1999; Pfeiffer and Wollheim, 1999). These are, in fact, the most recent of a series of significant events that have been seen on the Arctic Peninsula.'}]]\n",
            "[{'deep': 0.040313028108570126, 'learning': 0.040313028108570126, 'first': 0.030659096963137636, 'step': 0.040313028108570126, 'designing': 0.040313028108570126, 'algorithm': 0.040313028108570126, 'us': 0.023809523809523808, 'finite': 0.040313028108570126, 'number': 0.030659096963137636, 'discrete': 0.040313028108570126, 'method': 0.040313028108570126, 'used': 0.12093908432571038, 'classifier': 0.08062605621714025, 'take': 0.040313028108570126, 'set': 0.12093908432571038, 'model': 0.040313028108570126, 'second': 0.040313028108570126, 'stage': 0.040313028108570126, 'use': 0.08062605621714025, 'constraint': 0.1612521124342805, 'testing': 0.040313028108570126, 'input': 0.040313028108570126, 'data': 0.040313028108570126, 'generated': 0.040313028108570126, 'finally': 0.040313028108570126, 'different': 0.040313028108570126, 'implementation': 0.040313028108570126, 'test': 0.08062605621714025, 'output': 0.040313028108570126, 'correctness': 0.040313028108570126, 'though': 0.040313028108570126, 'important': 0.040313028108570126}, {'natural': 0.042328679513998636, 'language': 0.16931471805599455, 'processing': 0.042328679513998636, 'child': 0.042328679513998636, 'ability': 0.042328679513998636, 'acquire': 0.042328679513998636, 'information': 0.08465735902799727, 'many': 0.042328679513998636, 'reason': 0.042328679513998636, 'believe': 0.042328679513998636, 'brain': 0.16931471805599455, 'tell': 0.042328679513998636, 'u': 0.01942141121714476, 'thing': 0.042328679513998636, 'processed': 0.042328679513998636, 'image': 0.042328679513998636, 'basic': 0.042328679513998636, 'process': 0.042328679513998636, 'isnt': 0.042328679513998636, 'least': 0.042328679513998636, 'thats': 0.042328679513998636, 'view': 0.042328679513998636, 'neuroscientist': 0.042328679513998636, 'thomas': 0.042328679513998636, 'friedman': 0.042328679513998636, 'author': 0.042328679513998636, 'bestselling': 0.042328679513998636, 'psychology': 0.042328679513998636, 'scientific': 0.042328679513998636, 'psychological': 0.042328679513998636, 'evidence': 0.042328679513998636, 'coauthor': 0.042328679513998636, 'book': 0.042328679513998636}, {'computer': 0.03455402409306011, 'vision': 0.03455402409306011, 'system': 0.03455402409306011, 'monday': 0.03455402409306011, 'announced': 0.03455402409306011, 'uber': 0.03455402409306011, 'wa': 0.03455402409306011, 'partnering': 0.03455402409306011, 'officially': 0.03455402409306011, 'university': 0.06910804818612022, 'north': 0.026279225968403687, 'carolina': 0.03455402409306011, 'chapel': 0.03455402409306011, 'hill': 0.03455402409306011, 'become': 0.03455402409306011, 'first': 0.026279225968403687, 'startup': 0.03455402409306011, 'distributed': 0.03455402409306011, 'autonomous': 0.03455402409306011, 'driving': 0.03455402409306011, 'technology': 0.03455402409306011, 'announcement': 0.03455402409306011, 'followed': 0.03455402409306011, 'wave': 0.03455402409306011, 'buzz': 0.03455402409306011, 'ubers': 0.03455402409306011, 'success': 0.03455402409306011, 'california': 0.03455402409306011, 'recent': 0.026279225968403687, '100': 0.03455402409306011, 'million': 0.03455402409306011, 'grant': 0.03455402409306011, 'harvard': 0.03455402409306011, 'certainly': 0.03455402409306011, 'doe': 0.03455402409306011, 'bode': 0.03455402409306011, 'well': 0.03455402409306011, 'future': 0.03455402409306011, 'uberdriving': 0.03455402409306011, 'project': 0.10366207227918033, 'end': 0.03455402409306011, 'company': 0.03455402409306011, 'two': 0.03455402409306011, 'work': 0.03455402409306011, 'new': 0.03455402409306011, 'virtualreality': 0.03455402409306011}, {'history': 0.04576073460972826, 'egypt': 0.04576073460972826, 'geoecology': 0.04576073460972826, '66': 0.04576073460972826, '1077': 0.04576073460972826, '1083': 0.04576073460972826, '1998': 0.04576073460972826, 'identified': 0.04576073460972826, 'number': 0.03480221817437246, 'unusual': 0.04576073460972826, 'rare': 0.04576073460972826, 'geodesy': 0.04576073460972826, 'event': 0.09152146921945652, 'occurred': 0.04576073460972826, 'northern': 0.04576073460972826, 'coast': 0.04576073460972826, 'north': 0.03480221817437246, 'american': 0.04576073460972826, 'eurasian': 0.04576073460972826, 'sea': 0.04576073460972826, '1882': 0.04576073460972826, 'present': 0.04576073460972826, 'day': 0.03480221817437246, 'munro': 0.04576073460972826, 'crespo': 0.04576073460972826, '1999': 0.09152146921945652, 'pfeiffer': 0.04576073460972826, 'wollheim': 0.04576073460972826, 'fact': 0.04576073460972826, 'recent': 0.03480221817437246, 'series': 0.04576073460972826, 'significant': 0.04576073460972826, 'seen': 0.04576073460972826, 'arctic': 0.04576073460972826, 'peninsula': 0.04576073460972826}]\n",
            "Document 1:\n",
            "deep: 0.02432399503393523\n",
            "learning: 0.02432399503393523\n",
            "first: 0.018499025185303795\n",
            "step: 0.02432399503393523\n",
            "designing: 0.02432399503393523\n",
            "algorithm: 0.02432399503393523\n",
            "us: 0.014366143305918021\n",
            "finite: 0.02432399503393523\n",
            "number: 0.018499025185303795\n",
            "discrete: 0.02432399503393523\n",
            "method: 0.02432399503393523\n",
            "used: 0.07297198510180569\n",
            "classifier: 0.04864799006787046\n",
            "take: 0.02432399503393523\n",
            "set: 0.07297198510180569\n",
            "model: 0.02432399503393523\n",
            "second: 0.02432399503393523\n",
            "stage: 0.02432399503393523\n",
            "use: 0.04864799006787046\n",
            "constraint: 0.09729598013574092\n",
            "testing: 0.02432399503393523\n",
            "input: 0.02432399503393523\n",
            "data: 0.02432399503393523\n",
            "generated: 0.02432399503393523\n",
            "finally: 0.02432399503393523\n",
            "different: 0.02432399503393523\n",
            "implementation: 0.02432399503393523\n",
            "test: 0.04864799006787046\n",
            "output: 0.02432399503393523\n",
            "correctness: 0.02432399503393523\n",
            "though: 0.02432399503393523\n",
            "important: 0.02432399503393523\n",
            "\n",
            "Document 2:\n",
            "natural: 0.025342873920816213\n",
            "language: 0.10137149568326485\n",
            "processing: 0.025342873920816213\n",
            "child: 0.025342873920816213\n",
            "ability: 0.025342873920816213\n",
            "acquire: 0.025342873920816213\n",
            "information: 0.05068574784163243\n",
            "many: 0.025342873920816213\n",
            "reason: 0.025342873920816213\n",
            "believe: 0.025342873920816213\n",
            "brain: 0.10137149568326485\n",
            "tell: 0.025342873920816213\n",
            "u: 0.011627917088168328\n",
            "thing: 0.025342873920816213\n",
            "processed: 0.025342873920816213\n",
            "image: 0.025342873920816213\n",
            "basic: 0.025342873920816213\n",
            "process: 0.025342873920816213\n",
            "isnt: 0.025342873920816213\n",
            "least: 0.025342873920816213\n",
            "thats: 0.025342873920816213\n",
            "view: 0.025342873920816213\n",
            "neuroscientist: 0.025342873920816213\n",
            "thomas: 0.025342873920816213\n",
            "friedman: 0.025342873920816213\n",
            "author: 0.025342873920816213\n",
            "bestselling: 0.025342873920816213\n",
            "psychology: 0.025342873920816213\n",
            "scientific: 0.025342873920816213\n",
            "psychological: 0.025342873920816213\n",
            "evidence: 0.025342873920816213\n",
            "coauthor: 0.025342873920816213\n",
            "book: 0.025342873920816213\n",
            "\n",
            "Document 3:\n",
            "computer: 0.02071183369260065\n",
            "vision: 0.02071183369260065\n",
            "system: 0.02071183369260065\n",
            "monday: 0.02071183369260065\n",
            "announced: 0.02071183369260065\n",
            "uber: 0.02071183369260065\n",
            "wa: 0.02071183369260065\n",
            "partnering: 0.02071183369260065\n",
            "officially: 0.02071183369260065\n",
            "university: 0.0414236673852013\n",
            "north: 0.015751883380123177\n",
            "carolina: 0.02071183369260065\n",
            "chapel: 0.02071183369260065\n",
            "hill: 0.02071183369260065\n",
            "become: 0.02071183369260065\n",
            "first: 0.015751883380123177\n",
            "startup: 0.02071183369260065\n",
            "distributed: 0.02071183369260065\n",
            "autonomous: 0.02071183369260065\n",
            "driving: 0.02071183369260065\n",
            "technology: 0.02071183369260065\n",
            "announcement: 0.02071183369260065\n",
            "followed: 0.02071183369260065\n",
            "wave: 0.02071183369260065\n",
            "buzz: 0.02071183369260065\n",
            "ubers: 0.02071183369260065\n",
            "success: 0.02071183369260065\n",
            "california: 0.02071183369260065\n",
            "recent: 0.015751883380123177\n",
            "100: 0.02071183369260065\n",
            "million: 0.02071183369260065\n",
            "grant: 0.02071183369260065\n",
            "harvard: 0.02071183369260065\n",
            "certainly: 0.02071183369260065\n",
            "doe: 0.02071183369260065\n",
            "bode: 0.02071183369260065\n",
            "well: 0.02071183369260065\n",
            "future: 0.02071183369260065\n",
            "uberdriving: 0.02071183369260065\n",
            "project: 0.06213550107780195\n",
            "end: 0.02071183369260065\n",
            "company: 0.02071183369260065\n",
            "two: 0.02071183369260065\n",
            "work: 0.02071183369260065\n",
            "new: 0.02071183369260065\n",
            "virtualreality: 0.02071183369260065\n",
            "\n",
            "Document 4:\n",
            "history: 0.02774532876661411\n",
            "egypt: 0.02774532876661411\n",
            "geoecology: 0.02774532876661411\n",
            "66: 0.02774532876661411\n",
            "1077: 0.02774532876661411\n",
            "1083: 0.02774532876661411\n",
            "1998: 0.02774532876661411\n",
            "identified: 0.02774532876661411\n",
            "number: 0.02110103767543365\n",
            "unusual: 0.02774532876661411\n",
            "rare: 0.02774532876661411\n",
            "geodesy: 0.02774532876661411\n",
            "event: 0.05549065753322822\n",
            "occurred: 0.02774532876661411\n",
            "northern: 0.02774532876661411\n",
            "coast: 0.02774532876661411\n",
            "north: 0.02110103767543365\n",
            "american: 0.02774532876661411\n",
            "eurasian: 0.02774532876661411\n",
            "sea: 0.02774532876661411\n",
            "1882: 0.02774532876661411\n",
            "present: 0.02774532876661411\n",
            "day: 0.02110103767543365\n",
            "munro: 0.02774532876661411\n",
            "crespo: 0.02774532876661411\n",
            "1999: 0.05549065753322822\n",
            "pfeiffer: 0.02774532876661411\n",
            "wollheim: 0.02774532876661411\n",
            "fact: 0.02774532876661411\n",
            "recent: 0.02110103767543365\n",
            "series: 0.02774532876661411\n",
            "significant: 0.02774532876661411\n",
            "seen: 0.02774532876661411\n",
            "arctic: 0.02774532876661411\n",
            "peninsula: 0.02774532876661411\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install nltk\n",
        "!python -m nltk.downloader stopwords\n",
        "!python -m nltk.downloader wordnet\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the text generation pipeline with the desired model\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "generated_text1 = generator(\"deep learning\", max_length=100)\n",
        "generated_text2 = generator(\"natural language processing\", max_length=100)\n",
        "generated_text3 = generator(\"computer vision\", max_length=100)\n",
        "generated_text4= generator(\"history of egypt\", max_length=100)\n",
        "\n",
        "documents=[generated_text1,generated_text2,generated_text3,generated_text4]\n",
        "\n",
        "print(documents)\n",
        "\n",
        "\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "        ## cleaning the data\n",
        "    cleaned_data= re.sub(r'[^\\w\\s]','',text)\n",
        "\n",
        "    ## Normalization\n",
        "    normatized_text= cleaned_data.lower()\n",
        "\n",
        "    ## Tokenization\n",
        "    tokenized_text= normatized_text.split()\n",
        "\n",
        "    ## Lemmatization\n",
        "    lemm= WordNetLemmatizer()\n",
        "    lemmatized_text = [lemm.lemmatize(word) for word in tokenized_text]\n",
        "\n",
        "    ## Unique Words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    unique_words= [word for word in lemmatized_text if word not in stop_words]\n",
        "\n",
        "    return ' '.join(unique_words)\n",
        "\n",
        "# Preprocess each document\n",
        "preprocessed_documents =[]\n",
        "\n",
        "for doc in documents:\n",
        "    temp=doc[0]['generated_text']\n",
        "    processed_doc=preprocess_text(temp)\n",
        "    preprocessed_documents.append(processed_doc)\n",
        "\n",
        "# Function to calculate TF for a document\n",
        "def calculate_tf(document):\n",
        "\n",
        "    words = document.split()\n",
        "    word_count = Counter(words)\n",
        "    tf = {word: count/len(words) for word, count in word_count.items()}\n",
        "    return tf\n",
        "\n",
        "# Calculate TF for all preprocessed documents\n",
        "tf_documents = [calculate_tf(doc) for doc in preprocessed_documents]\n",
        "\n",
        "# Function to calculate IDF for a word across all documents\n",
        "\n",
        "def calculate_idf(word, all_documents):\n",
        "    num_documents_with_word = sum(1 for doc in all_documents if word in doc)\n",
        "    idf = np.log(len(all_documents) / (1 + num_documents_with_word)) + 1\n",
        "    return idf\n",
        "\n",
        "\n",
        "# Get unique words across all preprocessed documents\n",
        "all_words = set([word for doc in preprocessed_documents for word in doc.split()])\n",
        "# Calculate IDF for each word\n",
        "idf_values = {word: calculate_idf(word, preprocessed_documents) for word in all_words}\n",
        "\n",
        "# Function to calculate TF-IDF for a document\n",
        "def calculate_tfidf(tf, idf):\n",
        "    tfidf = {word: tf[word] * idf[word] for word in tf.keys()}\n",
        "    return tfidf\n",
        "\n",
        "# # Calculate TF-IDF for all preprocessed documents\n",
        "tfidf_documents = [calculate_tfidf(tf, idf_values) for tf in tf_documents]\n",
        "\n",
        "print(tfidf_documents)\n",
        "# Normalize TF-IDF values\n",
        "def normalize_tfidf(tfidf):\n",
        "    total_tfidf = sum(tfidf.values())\n",
        "    normalized_tfidf = {word: value / total_tfidf for word, value in tfidf.items()}\n",
        "    return normalized_tfidf\n",
        "\n",
        "normalized_tfidf_documents = [normalize_tfidf(tfidf) for tfidf in tfidf_documents]\n",
        "\n",
        "# Print normalized TF-IDF for each document\n",
        "for i, doc_tfidf in enumerate(normalized_tfidf_documents):\n",
        "    print(f\"Document {i+1}:\")\n",
        "    for word, tfidf in doc_tfidf.items():\n",
        "        print(f\"{word}: {tfidf}\")\n",
        "    print()"
      ]
    }
  ]
}